{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Collecting regex\n",
      "  Downloading regex-2021.3.17-cp38-cp38-manylinux2014_x86_64.whl (737 kB)\n",
      "\u001b[K     |████████████████████████████████| 737 kB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk) (4.59.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434678 sha256=85ff92b90d85ab8f49183d05c31bdf762b4326ec5fe229f746f2612cf9998059\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/ff/d5/7b/f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\n",
      "Successfully built nltk\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.5 regex-2021.3.17\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhvD8J3LM4r0",
    "outputId": "c712b57e-f97a-43fe-8c9f-66f341eaddc6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22vQ_kcqNiM3",
    "outputId": "62e1f618-d56f-4cbf-e7b3-1b9172330111"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We are seeing an increasing number of errors with our payment services. The issue has been reported by multiple users in the last 3 hours and this is affecting our revenue. We need to fix it immediately. ',\n",
       " 'We need to make improvements to our landing page to convey our new branding guidelines.',\n",
       " 'It looks like the issue is limited only to visa credit cards.',\n",
       " 'We need to schedule a product meeting to discuss the new set of features and the roadmap.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus  = [ \"We are seeing an increasing number of errors with our payment services. The issue has been reported by multiple users in the last 3 hours and this is affecting our revenue. We need to fix it immediately. \",\n",
    "            \"We need to make improvements to our landing page to convey our new branding guidelines.\",\n",
    "            \"It looks like the issue is limited only to visa credit cards.\",\n",
    "            \"We need to schedule a product meeting to discuss the new set of features and the roadmap.\",\n",
    "             ]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UKGUGh0MPUtO"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EFdt6VLgPZOC",
    "outputId": "d6bf5057-b2b7-4579-860e-cc0726ba4e79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['seeing increasing number errors payment services issue reported multiple users last hours affecting revenue need fix immediately',\n",
       "       'need make improvements landing page convey new branding guidelines',\n",
       "       'looks like issue limited visa credit cards',\n",
       "       'need schedule product meeting discuss new set features roadmap'],\n",
       "      dtype='<U128')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FxKaje4lDg-9",
    "outputId": "326d5795-3c41-44fd-f277-c6203d94c25b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.4.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 2.1 MB/s eta 0:00:01     |███████▎                        | 471 kB 2.1 MB/s eta 0:00:01     |███████████▉                    | 757 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 2.6 MB/s eta 0:00:01     |███████████████████████████████▌| 870 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.1-cp38-cp38-manylinux2010_x86_64.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 2.5 MB/s eta 0:00:01     |█▎                              | 133 kB 1.6 MB/s eta 0:00:02     |███▎                            | 327 kB 1.6 MB/s eta 0:00:02     |█████████▉                      | 993 kB 1.6 MB/s eta 0:00:02     |██████████████████████          | 2.2 MB 1.6 MB/s eta 0:00:01     |███████████████████████▏        | 2.3 MB 1.6 MB/s eta 0:00:01     |█████████████████████████▉      | 2.6 MB 2.5 MB/s eta 0:00:01     |██████████████████████████████▎ | 3.1 MB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2021.3.17)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893258 sha256=f247b7c52dfd9d365f093cc178245f136e0498039b87c9b7ab58e3020bdb5bcc\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/7b/78/f4/27d43a65043e1b75dbddaa421b573eddc67e712be4b1c80677\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, filelock, transformers\n",
      "Successfully installed filelock-3.0.12 sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "9hyqKu5XDoi-"
   },
   "outputs": [],
   "source": [
    "#import torch\n",
    "from transformers import TFBertModel, BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "sH6n5lcRDhnY"
   },
   "outputs": [],
   "source": [
    "# Initialize the tokenizer with a pretrained model\n",
    "#tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "agjmw1EJqwjo"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IEbJHosjDuPP",
    "outputId": "e6f0843a-be81-4925-cede-3fc64a9c263f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence tokens:  19\n"
     ]
    }
   ],
   "source": [
    "#get max_len\n",
    "length=[len(tokenizer.encode(x,add_special_tokens=True)) for x in norm_corpus ]\n",
    "\n",
    "#print(length)\n",
    "max_len=max(length)\n",
    "print('Max sentence tokens: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "GuOVJsmerFeM"
   },
   "outputs": [],
   "source": [
    "def encoding(sentences):\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                          sent,                      \n",
    "                          add_special_tokens = True,\n",
    "                          max_length = 30,           \n",
    "                          pad_to_max_length = True,\n",
    "                          return_attention_mask = True,   \n",
    "                          return_tensors = 'tf',   \n",
    "                          truncation=True\n",
    "                    )\n",
    "\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = tf.concat(input_ids, axis=0)\n",
    "    attention_masks = tf.concat(attention_masks, axis=0)\n",
    "\n",
    "    return input_ids,attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "6SUqM_Udsrhc"
   },
   "outputs": [],
   "source": [
    "def get_corpus_embedding(input_ids,attention_masks):\n",
    "    out = model(input_ids, attention_mask= attention_masks)\n",
    "    sentence_embedding = tf.reduce_mean(out[0], axis=1)\n",
    "    #sentence_embedding= out[1]\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "vyyTkLmDtR0J"
   },
   "outputs": [],
   "source": [
    "def get_user_embedding(user_query):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        user_query,                      \n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 30,           \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   \n",
    "                        return_tensors = 'tf',   \n",
    "                        truncation=True\n",
    "                   )\n",
    "    output= model(encoded_dict['input_ids'],encoded_dict['attention_mask'] )\n",
    "    user_query_embedding= tf.reduce_mean(output[0], axis=1)\n",
    "    user_query_embedding= tf.squeeze(user_query_embedding)\n",
    "    #user_query_embedding=output[1].squeeze()\n",
    "    return user_query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "def cal_cosine_sim(emb1,emb2):\n",
    "    output = 1 - spatial.distance.cosine(emb1,emb2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "qNBexazZwJCI"
   },
   "outputs": [],
   "source": [
    "def cal_cosine_sim2(emb1,emb2):\n",
    "    output = tf.keras.losses.cosine_similarity(emb1,emb2,axis=0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BjCuw-BqvXtO",
    "outputId": "c8503ad8-99ff-4fdf-aa51-9cb26744b6b9"
   },
   "outputs": [],
   "source": [
    "input_ids,attention_masks= encoding(norm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wrkL9dsOvJPG",
    "outputId": "5081ad09-dd31-4a45-f2af-529bb9570513"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#model = XLNetModel.from_pretrained('xlnet-base-cased', output_hidden_states=True)\n",
    "\n",
    "\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8kN-PN-vleo",
    "outputId": "cf960c29-38a2-4022-c00b-f5e5c76c46c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 768])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding= get_corpus_embedding(input_ids,attention_masks)\n",
    "sentence_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1zdAi9dJvwU2",
    "outputId": "9b03101e-b14a-4c66-d2fb-bfd1b0b5e560"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([768])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query= 'Payment issue'\n",
    "\n",
    "user_query_embedding= get_user_embedding(user_query)\n",
    "\n",
    "user_query_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHRKn_oqwHFE",
    "outputId": "f715ce0b-f2e7-4c5e-c22b-a52956b62247"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6625890731811523\n",
      "0.7352842688560486\n",
      "0.778020977973938\n",
      "0.6977018117904663\n"
     ]
    }
   ],
   "source": [
    "for each in sentence_embedding:\n",
    "  print(cal_cosine_sim(user_query_embedding, each))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELASTIC SEARCH INSERTION AND SEARCHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch\n",
      "  Downloading elasticsearch-7.11.0-py2.py3-none-any.whl (325 kB)\n",
      "\u001b[K     |███████████████████████████████▏| 317 kB 1.8 MB/s eta 0:00:01     |████████████████████████████▏   | 286 kB 1.8 MB/s eta 0:00:01     |████████████████████████████████| 325 kB 1.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from elasticsearch) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from elasticsearch) (1.26.4)\n",
      "Installing collected packages: elasticsearch\n",
      "Successfully installed elasticsearch-7.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host':'elasticsearch','port':9200}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getQuotes():\n",
    "    for line in corpus:\n",
    "        quote = line.strip().lower()\n",
    "        if (len(quote.split()) <= 510): # 510 IS THE MAX\n",
    "            vector = corpus\n",
    "            yield {\n",
    "                \"_index\": 'quotes',\n",
    "                \"quote\" : quote,\n",
    "                \"vector\" : vector\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, [])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bulk(client=es, actions = getQuotes(), chunk_size=1000, request_timeout = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME= 'conversation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'conversation'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_query = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"description_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 768\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "es.indices.create(index=INDEX_NAME, body=create_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [{\n",
    "    '_index': INDEX_NAME,\n",
    "    'description': sentence,\n",
    "} for sentence in norm_corpus]\n",
    "\n",
    "requests = []\n",
    "for i, doc in enumerate(docs):\n",
    "    request = doc\n",
    "    request['description_vector'] = sentence_embedding[i].numpy().tolist()\n",
    "    requests.append(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, [])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import helpers\n",
    "helpers.bulk(es, requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = {\n",
    "    \"size\": 2,\n",
    "    \"_source\": {\n",
    "        \"includes\": [\"description\"]\n",
    "    },\n",
    "    \"query\": {\n",
    "        \"script_score\": {\n",
    "            \"query\": {\n",
    "                \"match_all\": {}\n",
    "            },\n",
    "            \"script\": {\n",
    "                \"source\": \"cosineSimilarity(params.queryVector, 'description_vector') + 1.0\",\n",
    "                \"params\": {\n",
    "                    \"queryVector\": user_query_embedding.numpy().tolist()\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'took': 147,\n",
       " 'timed_out': False,\n",
       " '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0},\n",
       " 'hits': {'total': {'value': 4, 'relation': 'eq'},\n",
       "  'max_score': 1.778021,\n",
       "  'hits': [{'_index': 'conversation',\n",
       "    '_type': '_doc',\n",
       "    '_id': '6iiTQngBRItEpPcjP9V5',\n",
       "    '_score': 1.778021,\n",
       "    '_source': {'description': 'looks like issue limited visa credit cards'}},\n",
       "   {'_index': 'conversation',\n",
       "    '_type': '_doc',\n",
       "    '_id': '6SiTQngBRItEpPcjP9V5',\n",
       "    '_score': 1.7352842,\n",
       "    '_source': {'description': 'need make improvements landing page convey new branding guidelines'}}]}}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es.search(\n",
    "    index=INDEX_NAME,\n",
    "    body=search_query\n",
    ")\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Veamly Technical Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
